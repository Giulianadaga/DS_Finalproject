---
title: "Giuliana Daga-Final Project Yields"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r import data, message=FALSE, warning=FALSE, include=FALSE}
data_weather <- read_dta("Data/weather_profit_data.dta") 

# Correlation matrix
data_weather %>% 
 select(maize_dry_kg_perha_win, actual_pay,   contains("mean") & (contains("Rain")| contains("soil" )| contains("evapotranspiration") )) %>%
  GGally::ggcorr(., method = c("pairwise","pearson"), palette = "RdBu", hjust = .85, size = 3,
       layout.exp=2) +
  scale_fill_viridis_c(option="magma")


data_weather <- data_weather %>%
 select(-actual_pay) %>%
     drop_na(maize_dry_kg_perha_win)

skimr::skim(data_weather)
  
```

```{r split data, include=FALSE}

set.seed(14786) # We set a seed so we can reproduce the random split
splits <- initial_split(data_weather,prop = .75, strata = maize_dry_kg_perha_win)
train_data = training(splits) # Use 75% of the data as training data 
test_data = testing(splits) # holdout 25% as test data 

dim(train_data)

```

```{r recipe, include=FALSE}
rcp <- 
  recipe(maize_dry_kg_perha_win ~  .,train_data) %>% 
    step_meanimpute(all_numeric()) %>%  # Impute all missings in numerics with means
  step_modeimpute(all_nominal()) %>% # Impute all missings in factos with the mode
  step_dummy(all_nominal()) %>% # Convert all factor variables into dummies which is better for the model
  step_log(maize_dry_kg_perha_win, offset = 1) %>% # Log the skewed  variables
  step_range(all_numeric()) %>%  # Normalize scale
  prep()


# Apply the recipe to the training and test data
train_data2 <- bake(rcp,train_data)
test_data2 <- bake(rcp,test_data) 

head(train_data2)

```

```{r create folds set cross validation, include=FALSE}
# Set a seed for replication purposes 
set.seed(1988) 

# Partition the data into 5 equal folds
folds <- createFolds(train_data2$maize_dry_kg_perha_win, k = 5) 
sapply(folds,length)

# Set up valiation conditions
control_conditions <- 
  trainControl(method='cv', # K-fold cross validation
               index = folds # The indices for our folds (so they are always the same)
  )

```


```{r regression tree}
#tune_cart <- expand.grid(cp = c(0.0000050281)) # Complexity Parameter (how "deep" our trees should grow)
mod_cart <-
   train(maize_dry_kg_perha_win ~ .,            # Equation (outcome and everything else)
        data=train_data2,    # Training data 
        method = "rpart",    # Regression tree
        metric = "RMSE",     # mean squared error
        #eGrid = tune_cart, # Tuning parameters
        trControl = control_conditions # Cross validation conditions
  )

mod_cart

```

2) Random forest:

```{r}

mod_rf <-
  train(maize_dry_kg_perha_win ~ ., # Equation (outcome and everything else)
        data=train_data2, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "RMSE",     # mean squared error
        trControl = control_conditions
  )

mod_rf

```

3) Polynomial boundery

```{r}
mod_svm_poly <-
  train(maize_dry_kg_perha_win ~ .,
        data=train_data2, # Training data 
        method = "svmPoly", # SVM with a polynomial Kernel
        metric = "RMSE", # area under the curve
        trControl = control_conditions
  )

mod_svm_poly
```



```{r}

# Different values of the tuning parameter that I want to try.
knn_tune = expand.grid(k = c(1,3,10,50))

mod_knn <-
  train(maize_dry_kg_perha_win ~ .,           # Equation (outcome and everything else)
        data=train_data2,  # Training data 
        method = "knn",    # K-Nearest Neighbors Algorithm
        metric = "RMSE",   # mean squared error
        trControl = control_conditions, # Cross validation conditions
        tuneGrid = knn_tune # Vary the tuning parameter K 
  )

```




Compare all models

```{r}
# Organize all model imputs as a list.
mod_list <-
  list(
    cart = mod_cart,
    rf = mod_rf, 
    svm_poly = mod_svm_poly,
    knn = mod_knn
  )

# Resamples allows us to compare model output
resamples(mod_list)


# Graphs
dotplot(resamples(mod_list),metric = "RMSE")
dotplot(resamples(mod_list),metric = "Rsquared")

```



```{r}

# Which model performed better?
pred <- predict(mod_knn,newdata = test_data2)
mse = sum((test_data2$maize_dry_kg_perha_win-pred)^2)/nrow(test_data2)
rmse_score = sqrt(mse)
rmse_score

# Generate a prediction on our test data. 
pred <- predict(mod_knn,newdata = test_data2)


# Organize as a data frame
performance = tibble(truth=test_data2$maize_dry_kg_perha_win,estimate = pred)

# Calculate performance metrics
bind_rows(
  performance %>% rmse(truth,estimate), # Root Mean Squared Error
  performance %>% rsq(truth,estimate) # R Squared
)

```

## Variable importance: 

a. Regression tree: 
```{r}
vi_plot<- 
  vip(mod_cart, # Machine learning model
      train = train_data2, # Training data 
      method="permute", # permuted importance
      nsim = 10, # number of times to impute
      geom = "boxplot", # Type of plot 
      target = "maize_dry_kg_perha_win", # outcome
      metric = "rsquared",
      pred_wrapper = predict)

# Plot VIP
vi_plot
```

```{r}

mod_cart_selected <-
  train( maize_dry_kg_perha_win ~  humidity_Germination_mean + Rain_PreHarvest_sd + Rain_PreHarvest_mean + Rain_PreHarvest_max + Rain_Germination_sd + Rain_Germination_mean + Rain_Germination_max + Rain_Germination_sd + Rain_Flowering_sd + Rain_Flowering_mean + Rain_Flowering_max,
        data=train_data2, # Training data 
        method = "rpart", # random forest (ranger is much faster than rf)
        metric = "RMSE", # area under the curve
        trControl = control_conditions
  )

mod_cart_selected 
 
```

```{r}

train_data3 <- 
  train_data2 %>% 
  mutate(
    # Probability predictions 
    yields_pred = predict(mod_cart_selected)
  )  

surrogate_tree <-
  
  # Decision tree model (directly us the model rather than use the implementation in caret)
  rpart::rpart(
    # Main selected model. The outcome is now the predicted probabilities from
    # the RF model
    yields_pred ~ humidity_Germination_mean + Rain_PreHarvest_sd + Rain_PreHarvest_mean + Rain_PreHarvest_max + Rain_Germination_sd + Rain_Germination_mean + Rain_Germination_max + Rain_Germination_sd + Rain_Flowering_sd + Rain_Flowering_mean + Rain_Flowering_max,
        data=train_data2, # Training data 
    
    # Data is being passed by the pipe
    data = train_data3,
    
    # Note that we can control the depth of the tree
    # a deeper tree increase fit but reduces interpretability
    control = rpart::rpart.control(maxdepth = 10)
  )


rattle::fancyRpartPlot(surrogate_tree,sub="",type=5)
```





```{r}
# PDP for age
humidity_germ_pdp <- partial(mod_cart_selected, pred.var = "humidity_Germination_mean", plot = TRUE,prob=T,
                   grid.resolution = 20, # choosing less points makes it run quicker
                   plot.engine = "ggplot2")

rain_germ_pdp <- partial(mod_cart_selected, pred.var = "Rain_PreHarvest_sd", plot = TRUE,prob=T,
                   grid.resolution = 20, # choosing less points makes it run quicker
                   plot.engine = "ggplot2")

humidity_germ_pdp + rain_germ_pdp
```



b. Random forest:


```{r}
vi_plot<- 
  vip(mod_rf, # Machine learning model
      train = train_data2, # Training data 
      method="permute", # permuted importance
      nsim = 10, # number of times to impute
      geom = "boxplot", # Type of plot 
      target = "maize_dry_kg_perha_win", # outcome
      metric = "rsquared",
      pred_wrapper = predict)

# Plot VIP
vi_plot
```


```{r}

mod_rf_selected <-
  train( maize_dry_kg_perha_win ~  humidity_Germination_mean + soil_heat_Vegetation_mean + humidity_Vegetation_mean + Rain_Vegetation_max + soil_heat_Germination_mean + soil_heat_Germination_mean + soil_heat_Flowering_mean + humidity_Flowering_mean + humidity_Germination_max + humidity_PreHarvest_mean + evapotranspiration_Flowering_mean,
        data=train_data2, # Training data 
        method = "ranger", # random forest (ranger is much faster than rf)
        metric = "RMSE", # area under the curve
        trControl = control_conditions
  )

mod_rf_selected 
 
```

```{r}

train_data4 <- 
  train_data2 %>% 
  mutate(
    # Probability predictions 
    yields_pred = predict(mod_rf_selected)
  )  

surrogate_tree2 <-
  
  # Decision tree model (directly us the model rather than use the implementation in caret)
  rpart::rpart(
    # Main selected model. The outcome is now the predicted probabilities from
    # the RF model
    yields_pred ~ humidity_Germination_mean + soil_heat_Vegetation_mean + humidity_Vegetation_mean + Rain_Vegetation_max + soil_heat_Germination_mean + soil_heat_Germination_mean + soil_heat_Flowering_mean + humidity_Flowering_mean + humidity_Germination_max + humidity_PreHarvest_mean + evapotranspiration_Flowering_mean,
    
    # Data is being passed by the pipe
    data = train_data4,
    
    # Note that we can control the depth of the tree
    # a deeper tree increase fit but reduces interpretability
    control = rpart::rpart.control(maxdepth = 10)
  )

rattle::fancyRpartPlot(surrogate_tree2,sub="",type=5)
```





```{r}
# PDP for age
humidity_germ_pdp <- partial(mod_rf_selected, pred.var = "humidity_Germination_mean", plot = TRUE,prob=T,
                   grid.resolution = 20, # choosing less points makes it run quicker
                   plot.engine = "ggplot2")

rain_germ_pdp <- partial(mod_rf_selected, pred.var = "humidity_Vegetation_mean", plot = TRUE,prob=T,
                   grid.resolution = 20, # choosing less points makes it run quicker
                   plot.engine = "ggplot2")

rain_germ_pdp <- partial(mod_rf_selected, pred.var = "humidity_Vegetation_mean", plot = TRUE,prob=T,
                   grid.resolution = 20, # choosing less points makes it run quicker
                   plot.engine = "ggplot2")

humidity_germ_pdp + rain_germ_pdp
```



```{r}

train_data4 <- 
  train_data2 %>% 
  mutate(
    # Probability predictions 
    yields_pred = predict(mod_rf)
  )  

surrogate_tree2 <-
  
  # Decision tree model (directly us the model rather than use the implementation in caret)
  rpart::rpart(
    # Main selected model. The outcome is now the predicted probabilities from
    # the RF model
    yields_pred ~ Humidity_Germination + Rain_Germination + Rain_Vegetation+ Rain_Flowering + Rain_PreHarvest + Humidity_Flowering + Humidity_PreHarvest + Humidity_Vegetation + evapotranspiration_Germination + evapotranspiration_Flowering,
    
    # Data is being passed by the pipe
    data = train_data4,
    
    # Note that we can control the depth of the tree
    # a deeper tree increase fit but reduces interpretability
    control = rpart::rpart.control(maxdepth = 6)
  )

rattle::fancyRpartPlot(surrogate_tree,sub="",type=2)
```






